{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# what\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Cell-Fuzzy-Labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single-Cell Fuzzy Labels Concept](Single_Cell_Fuzzy_Labels/logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `Single-Cell-Fuzzy-Labels` library enhances single-cell RNA-seq data analysis by enabling label transfer from a reference dataset to new data using a K-nearest neighbors (KNN) approach. It leverages pre-trained models and advanced language models like GPT-3.5 or 4 to align disparate label sets, improving interpretability and integration. The library also offers metrics to evaluate the label transfer's effectiveness.\n",
    "\n",
    "## Streamlining Label Equivalence and Scoring with OpenAI API and GPT-4\n",
    "\n",
    "To quantify the success of label transfer, we use a centroid-based consensus strategy from our nearest neighbor method. The challenge lies in the lack of a universal standard for label sets, often leading to a mismatch between manual annotations and our predicted labels. For example, manual annotations might label cells as 'neurons', while our method could predict more specific types like 'pyramidal neurons' or 'interneurons'.\n",
    "\n",
    "We define the two label sets involved as:\n",
    "\n",
    "- **Existing Labels (E):** The original labels in the dataset, typically from manual annotations or established knowledge.\n",
    "- **Predicted Labels (P):** The labels our method predicts.\n",
    "\n",
    "Mathematically, we represent these sets as:\n",
    "\n",
    "$$\n",
    "\\text{Existing Labels (E)} = \\{e_1, e_2, e_3, \\ldots, e_n\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Predicted Labels (P)} = \\{p_1, p_2, p_3, \\ldots, p_m\\}\n",
    "$$\n",
    "\n",
    "Comparing **Existing Labels (E)** with **Predicted Labels (P)** is challenging due to the need to categorize complex biological processes and the inconsistencies in label detail and standards. For example, a broad category like 'basal' in **E** (notated as $e_i$) may equate to multiple subtypes in **P** (notated as $\\{p_j, p_k, \\ldots\\}$), requiring adaptable equivalence mapping strategies. Moreover, the same cell type might have different names, such as 'Type I Pneumocytes', 'Type I alveolar cells', or 'squamous alveolar cells', with variations in terms, spelling, and capitalization, complicating computational comparison despite being understandable to experts.\n",
    "\n",
    "\n",
    "**Label Mapping and Evaluation:**\n",
    "\n",
    "To compare the predicted label set \\( P \\) with the existing set \\( E \\), we define a mapping function \\( f: P \\rightarrow E \\). This function assigns each predicted label \\( p \\in P \\) to an existing label \\( e \\in E \\), allowing for a consistent comparison. For instance, 'neuron' in \\( P \\) might map to 'pyramidal neuron' in \\( E \\), based on criteria like hierarchy or linguistic similarity.\n",
    "\n",
    "The `fuzz1_score` evaluates this mapping's accuracy, akin to the F1 score, using the formula:\n",
    "\n",
    "$$\n",
    "\\text{fuzz1\\textunderscore score} = 2 \\times \\frac{(\\text{Precision} \\times \\text{Recall})}{(\\text{Precision} + \\text{Recall})}\n",
    "$$\n",
    "\n",
    "\n",
    "Precision is the ratio of correct predictions to total predictions, while Recall is the ratio of correct predictions to total existing labels. For example, correctly mapping 'neuron' to 'pyramidal neuron' and 'interneuron' in \\( E \\) counts as true positives. Incorrect mappings are false positives, and missed mappings are false negatives.\n",
    "\n",
    "The `fuzz1_score` thus quantifies the mapping's precision and recall, offering a concise measure of its effectiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install Single_Cell_Fuzzy_Labels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the `Single-Cell-Fuzzy-Labels` library in your single-cell RNA-seq data analysis, follow these steps:\n",
    "\n",
    "1. Install the library using pip:\n",
    "   ```sh\n",
    "   pip install Single_Cell_Fuzzy_Labels\n",
    "   ```\n",
    "\n",
    "2. Import the library in your Python environment:\n",
    "   ```python\n",
    "   from Single_Cell_Fuzzy_Labels.core import *\n",
    "   ```\n",
    "\n",
    "Optional steps before label transfer:\n",
    "\n",
    "3. Download pre-trained embeddings from cellxgene:\n",
    "   ```python\n",
    "   embeddings = download_embeddings(cellxgene_url)\n",
    "   ```\n",
    "\n",
    "   Or\n",
    "\n",
    "   Embed your own dataset using a foundation model such as UCE or scGPT:\n",
    "   ```python\n",
    "   dataset_embeddings = embed_dataset(your_dataset, model='UCE')\n",
    "   ```\n",
    "\n",
    "4. Assess embedding quality using Single-cell Integration Benchmarking (scIB):\n",
    "   ```python\n",
    "   quality_metrics = assess_embedding_quality(dataset_embeddings)\n",
    "   ```\n",
    "\n",
    "5. Prepare your reference dataset with well-annotated labels.\n",
    "\n",
    "6. Use the `transfer_labels` function to transfer labels from the reference dataset to your new query data:\n",
    "   ```python\n",
    "   transferred_labels = transfer_labels(query_data, reference_data)\n",
    "   ```\n",
    "\n",
    "7. Evaluate the label transfer quality using the provided metrics:\n",
    "   ```python\n",
    "   evaluate_transfer(transferred_labels)\n",
    "   ```\n",
    "\n",
    "For more detailed usage instructions and examples, refer to the documentation and the tutorials included in the GitHub repository.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
